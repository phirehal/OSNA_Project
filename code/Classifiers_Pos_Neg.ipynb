{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "import os\n",
    "import unidecode\n",
    "from textblob import TextBlob\n",
    "import random\n",
    "import numpy as np\n",
    "import io\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import re\n",
    "import hashlib\n",
    "from collections import Counter\n",
    "import glob\n",
    "import io\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subdirectories are:['test', 'train']\n"
     ]
    }
   ],
   "source": [
    "path = 'manual_labels'\n",
    "print('subdirectories are:' + str(os.listdir(path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_files(path):\n",
    "    return sorted([path+ os.sep +f for f in os.listdir(path) if f.endswith(\".txt\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 79 positive and 33 negative training files\n",
      "found 26 positive and 7 negative test files\n",
      "first positive file: manual_labels/train/pos/1.txt\n",
      "first negative file: manual_labels/train/neg/102.txt\n"
     ]
    }
   ],
   "source": [
    "pos_train_files = get_files(path + os.sep + 'train' + os.sep + 'pos')\n",
    "neg_train_files = get_files(path + os.sep + 'train' + os.sep + 'neg')\n",
    "all_train_files = pos_train_files + neg_train_files\n",
    "\n",
    "pos_test_files = get_files(path + os.sep + 'test' + os.sep + 'pos')\n",
    "neg_test_files = get_files(path + os.sep + 'test' + os.sep + 'neg')\n",
    "all_test_files = pos_test_files + neg_test_files\n",
    "\n",
    "print('found %d positive and %d negative training files' %\n",
    "      (len(pos_train_files), len(neg_train_files)))\n",
    "\n",
    "print('found %d positive and %d negative test files' %\n",
    "      (len(pos_test_files), len(neg_test_files)))\n",
    "\n",
    "print('first positive file: %s' % pos_train_files[0])\n",
    "print('first negative file: %s' % neg_train_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = []\n",
    "for pos_file in pos_train_files:\n",
    "    with open(pos_file, 'r') as content_file:\n",
    "        content = content_file.read()\n",
    "    train.append((unicode(content, 'unicode-escape'),'pos'))\n",
    "    \n",
    "for neg_file in neg_train_files:\n",
    "    with open(neg_file, 'r') as content_file:\n",
    "        content = content_file.read()\n",
    "    train.append((unicode(content, 'unicode-escape'),'neg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cl = NaiveBayesClassifier(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = []\n",
    "for pos_file in pos_test_files:\n",
    "    with open(pos_file, 'r') as content_file:\n",
    "        content = content_file.read()\n",
    "    test.append((unicode(content, 'unicode-escape'),'pos'))\n",
    "    \n",
    "for neg_file in neg_test_files:\n",
    "    with open(neg_file, 'r') as content_file:\n",
    "        content = content_file.read()\n",
    "    test.append((unicode(content, 'unicode-escape'),'neg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8181818181818182"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just test the accuracy score of the Naivebayes classifier\n",
    "cl.accuracy(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "            contains(ok) = True              neg : pos    =      8.6 : 1.0\n",
      "           contains(All) = True              neg : pos    =      8.6 : 1.0\n",
      "          contains(oily) = True              neg : pos    =      7.1 : 1.0\n",
      "          contains(With) = True              neg : pos    =      7.1 : 1.0\n",
      "         contains(still) = True              neg : pos    =      6.1 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# test the most informative features\n",
    "cl.show_informative_features(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_evaluation (pairs, pos_cls='pos', verbose=False):\n",
    "    N = len(pairs)\n",
    "    (ctr,correct, tp, tn, fp,fn) = (0,0,0,0,0,0)\n",
    "    for (predicted, actual) in pairs:\n",
    "        ctr += 1\n",
    "        if predicted == actual:\n",
    "            correct += 1\n",
    "            if actual == pos_cls:\n",
    "                tp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        else:\n",
    "            if actual == pos_cls:\n",
    "                fp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "    (accuracy, precision, recall) = (float(correct)/N,float(tp)/(tp + fp),float(tp)/(tp + fn))\n",
    "    if verbose:\n",
    "        print_results(precision, recall, accuracy, pos_cls)\n",
    "    return (accuracy, precision, recall)\n",
    "\n",
    "def print_results (precision, recall, accuracy, pos_cls):\n",
    "    banner =  'Evaluation with pos_cls = %s' % pos_cls\n",
    "    print \n",
    "    print (banner)\n",
    "    print ('=' + '*' + len(banner))\n",
    "    print ('Precision',precision*100)\n",
    "    print ('Recall',recall*100)\n",
    "    print ('Accuracy',accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8181818181818182, 0.14285714285714285, 1.0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = [(cl.classify(predicted), actual)\n",
    "            for (predicted, actual) in test]\n",
    "do_evaluation (pairs)\n",
    "do_evaluation (pairs, pos_cls='neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 3 and last 3 labels are: [1 1 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "def get_true_labels(file_names):\n",
    "    labels = []\n",
    "    for f in file_names:\n",
    "        if 'pos' in f:\n",
    "            labels.append(1);\n",
    "        else:\n",
    "            labels.append(0);\n",
    "    return np.array(labels)\n",
    "\n",
    "labels = get_true_labels(all_train_files)\n",
    "print('first 3 and last 3 labels are: %s' % str(labels[[1,2,3,-3,-2,-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Great place to have dinner with my best friend. I have been in New York for 6 years and I came from Shanghai. I feel Yaso Tangbao is the closest taste to my hometown. We ordered Yaso Pork Soup Dumplings, Pork And Bok Choy Wonton, Spicy Diced Pork Noodle , Chicken Curry Noodle Soup, Braised Pork Meatballs Over Rice With Eggs, Wine Chicken, Garlic Cucumber Salad and Spicy Cabbage Salad. They all taste so local and I love the environment very much. Strongly recommend!!!'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def file2string(filename):\n",
    "    return io.open(filename, encoding='utf8').readlines()[0]\n",
    "    \n",
    "file2string(pos_train_files[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return filter(None,re.split(r\"[\\W]\",text.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize_with_punct(text):\n",
    "    tokens = re.findall(r\"\\w+|\\S\", text.lower(),flags = re.L)\n",
    "    return [t.encode('utf-8').decode('utf-8').encode('utf-8') for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_with_not(text):\n",
    "    tokens = tokenize_with_punct(text)\n",
    "    for i,token in enumerate(tokens):\n",
    "        if (token == 'not'):\n",
    "            if (i+1 <= len(tokens)-1):\n",
    "                tokens[i+1] = token + '_' + tokens[i+1]\n",
    "            if (i+2 <= len(tokens)-1):\n",
    "                tokens[i+2] = token + '_' + tokens[i+2]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix represents 112 documents with 2294 features\n",
      "first doc has terms:\n",
      "[46, 65, 75, 84, 87, 98, 103, 105, 110, 111, 139, 151, 160, 193, 207, 216, 231, 246, 263, 270, 284, 288, 304, 333, 355, 380, 391, 414, 514, 515, 589, 612, 614, 615, 660, 763, 765, 780, 783, 817, 825, 841, 894, 915, 944, 959, 975, 1004, 1007, 1036, 1108, 1118, 1125, 1172, 1247, 1249, 1266, 1287, 1293, 1301, 1307, 1332, 1360, 1388, 1414, 1475, 1477, 1592, 1639, 1678, 1781, 1834, 1922, 1956, 2002, 2004, 2006, 2007, 2014, 2022, 2046, 2069, 2080, 2124, 2147, 2200, 2206, 2209, 2223, 2230, 2234, 2244, 2251, 2285]\n"
     ]
    }
   ],
   "source": [
    "def do_vectorize(filenames, tokenizer_fn=tokenize, min_df=1,\n",
    "                 max_df=1., binary=True,stop_words=None, ngram_range=(1,1)):\n",
    "    if type(filenames) is np.ndarray:\n",
    "        filenames = filenames.tolist()\n",
    "    vec = CountVectorizer(input='content',tokenizer=tokenizer_fn, min_df=min_df,\n",
    "                 max_df=max_df, binary=binary, ngram_range=ngram_range,stop_words=stop_words)\n",
    "    data  = []\n",
    "    for fil in filenames:\n",
    "        f = open(fil, 'r')\n",
    "        data.append(unicode(f.read(), 'unicode-escape'))\n",
    "    return vec.fit_transform(data),vec\n",
    "    \n",
    "matrix, vec = do_vectorize(all_train_files)\n",
    "print ('matrix represents %d documents with %d features' % (matrix.shape[0], matrix.shape[1]))\n",
    "print('first doc has terms:\\n%s' % (str(sorted(matrix[0].nonzero()[1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'00',\n",
       " u'1',\n",
       " u'10',\n",
       " u'100th',\n",
       " u'11',\n",
       " u'1130am',\n",
       " u'11am',\n",
       " u'11pm',\n",
       " u'12',\n",
       " u'14',\n",
       " u'140',\n",
       " u'15',\n",
       " u'16',\n",
       " u'18',\n",
       " u'192',\n",
       " u'1st',\n",
       " u'2',\n",
       " u'20',\n",
       " u'21',\n",
       " u'24',\n",
       " u'25',\n",
       " u'28',\n",
       " u'2nd',\n",
       " u'3',\n",
       " u'30',\n",
       " u'30pm',\n",
       " u'36',\n",
       " u'3pm',\n",
       " u'4',\n",
       " u'400',\n",
       " u'45pm',\n",
       " u'5',\n",
       " u'50',\n",
       " u'5ish',\n",
       " u'5pm',\n",
       " u'6',\n",
       " u'7',\n",
       " u'75',\n",
       " u'7pm',\n",
       " u'8',\n",
       " u'875',\n",
       " u'8pm',\n",
       " u'9',\n",
       " u'95',\n",
       " u'_',\n",
       " u'___________',\n",
       " u'a',\n",
       " u'a4',\n",
       " u'able',\n",
       " u'about',\n",
       " u'absolutely',\n",
       " u'abundance',\n",
       " u'accepts',\n",
       " u'accessible',\n",
       " u'accidentally',\n",
       " u'accompanying',\n",
       " u'accustomed',\n",
       " u'achar',\n",
       " u'acquire',\n",
       " u'across',\n",
       " u'act',\n",
       " u'active',\n",
       " u'actual',\n",
       " u'actually',\n",
       " u'add',\n",
       " u'added',\n",
       " u'addition',\n",
       " u'additional',\n",
       " u'admit',\n",
       " u'adorable',\n",
       " u'advise',\n",
       " u'aesthetics',\n",
       " u'affordability',\n",
       " u'affordable',\n",
       " u'aficionados',\n",
       " u'after',\n",
       " u'afternoon',\n",
       " u'afterwards',\n",
       " u'again',\n",
       " u'against',\n",
       " u'ago',\n",
       " u'agree',\n",
       " u'ahead',\n",
       " u'ahhh',\n",
       " u'air',\n",
       " u'aka',\n",
       " u'alcohol',\n",
       " u'all',\n",
       " u'allergic',\n",
       " u'allow',\n",
       " u'allows',\n",
       " u'almonds',\n",
       " u'almost',\n",
       " u'alone',\n",
       " u'along',\n",
       " u'aloo',\n",
       " u'already',\n",
       " u'alright',\n",
       " u'also',\n",
       " u'although',\n",
       " u'always',\n",
       " u'am',\n",
       " u'amalgam',\n",
       " u'amazing',\n",
       " u'ambiance',\n",
       " u'americanized',\n",
       " u'americans',\n",
       " u'among',\n",
       " u'amongst',\n",
       " u'amount',\n",
       " u'an',\n",
       " u'and',\n",
       " u'angeles',\n",
       " u'angels',\n",
       " u'annoying',\n",
       " u'another',\n",
       " u'answered',\n",
       " u'any',\n",
       " u'anymore',\n",
       " u'anyone',\n",
       " u'anything',\n",
       " u'anyway',\n",
       " u'apart',\n",
       " u'app',\n",
       " u'apparently',\n",
       " u'appeals',\n",
       " u'appetizer',\n",
       " u'appetizers',\n",
       " u'applaud',\n",
       " u'applied',\n",
       " u'appreciate',\n",
       " u'approachable',\n",
       " u'appropriately',\n",
       " u'apps',\n",
       " u'arcadia',\n",
       " u'are',\n",
       " u'area',\n",
       " u'aren',\n",
       " u'arm',\n",
       " u'around',\n",
       " u'arrived',\n",
       " u'arriving',\n",
       " u'artificial',\n",
       " u'as',\n",
       " u'asia',\n",
       " u'asian',\n",
       " u'ask',\n",
       " u'asked',\n",
       " u'asks',\n",
       " u'assorted',\n",
       " u'assortment',\n",
       " u'at',\n",
       " u'ate',\n",
       " u'atmo',\n",
       " u'atmosphere',\n",
       " u'atop',\n",
       " u'attached',\n",
       " u'attend',\n",
       " u'attentive',\n",
       " u'attitude',\n",
       " u'authentic',\n",
       " u'authenticity',\n",
       " u'automatically',\n",
       " u'available',\n",
       " u'ave',\n",
       " u'average',\n",
       " u'avocardo',\n",
       " u'awaiting',\n",
       " u'aware',\n",
       " u'away',\n",
       " u'awesome',\n",
       " u'awhile',\n",
       " u'awkward',\n",
       " u'b',\n",
       " u'back',\n",
       " u'bacon',\n",
       " u'bad',\n",
       " u'bae',\n",
       " u'bag',\n",
       " u'balls',\n",
       " u'bamboo',\n",
       " u'bang',\n",
       " u'bao',\n",
       " u'baos',\n",
       " u'bar',\n",
       " u'barely',\n",
       " u'base',\n",
       " u'basically',\n",
       " u'basil',\n",
       " u'basket',\n",
       " u'baskets',\n",
       " u'bat',\n",
       " u'batches',\n",
       " u'be',\n",
       " u'bean',\n",
       " u'beans',\n",
       " u'beautiful',\n",
       " u'because',\n",
       " u'become',\n",
       " u'beef',\n",
       " u'been',\n",
       " u'beers',\n",
       " u'beet',\n",
       " u'before',\n",
       " u'beg',\n",
       " u'begun',\n",
       " u'behind',\n",
       " u'being',\n",
       " u'believe',\n",
       " u'bellied',\n",
       " u'belly',\n",
       " u'beloved',\n",
       " u'below',\n",
       " u'bench',\n",
       " u'besides',\n",
       " u'best',\n",
       " u'better',\n",
       " u'beware',\n",
       " u'bf',\n",
       " u'big',\n",
       " u'bigger',\n",
       " u'biggest',\n",
       " u'bill',\n",
       " u'bin',\n",
       " u'bins',\n",
       " u'birthday',\n",
       " u'biryani',\n",
       " u'biryanj',\n",
       " u'bit',\n",
       " u'bite',\n",
       " u'bites',\n",
       " u'biting',\n",
       " u'bitter',\n",
       " u'black',\n",
       " u'blame',\n",
       " u'bland',\n",
       " u'blindly',\n",
       " u'block',\n",
       " u'blocks',\n",
       " u'blow',\n",
       " u'boiled',\n",
       " u'boisterous',\n",
       " u'bok',\n",
       " u'bolognese',\n",
       " u'bomb',\n",
       " u'bone',\n",
       " u'bonus',\n",
       " u'book',\n",
       " u'bookmarked',\n",
       " u'bordering',\n",
       " u'boring',\n",
       " u'boss',\n",
       " u'both',\n",
       " u'bother',\n",
       " u'bothering',\n",
       " u'bottom',\n",
       " u'bowl',\n",
       " u'braised',\n",
       " u'brawl',\n",
       " u'bread',\n",
       " u'breads',\n",
       " u'break',\n",
       " u'breaking',\n",
       " u'breath',\n",
       " u'briganti',\n",
       " u'brings',\n",
       " u'brink',\n",
       " u'broccoli',\n",
       " u'broke',\n",
       " u'broken',\n",
       " u'brooklyn',\n",
       " u'broth',\n",
       " u'brought',\n",
       " u'brunch',\n",
       " u'brushed',\n",
       " u'bs',\n",
       " u'bucatini',\n",
       " u'buck',\n",
       " u'buds',\n",
       " u'buffet',\n",
       " u'buffett',\n",
       " u'bullshit',\n",
       " u'bumping',\n",
       " u'bun',\n",
       " u'bunch',\n",
       " u'buns',\n",
       " u'burn',\n",
       " u'burned',\n",
       " u'burning',\n",
       " u'business',\n",
       " u'bustles',\n",
       " u'busy',\n",
       " u'but',\n",
       " u'butter',\n",
       " u'by',\n",
       " u'byob',\n",
       " u'c',\n",
       " u'cabbage',\n",
       " u'cake',\n",
       " u'cakes',\n",
       " u'call',\n",
       " u'called',\n",
       " u'calling',\n",
       " u'came',\n",
       " u'can',\n",
       " u'cant',\n",
       " u'car',\n",
       " u'carbonara',\n",
       " u'card',\n",
       " u'cards',\n",
       " u'care',\n",
       " u'careful',\n",
       " u'carpet',\n",
       " u'carrots',\n",
       " u'carry',\n",
       " u'case',\n",
       " u'cash',\n",
       " u'cashier',\n",
       " u'cashiers',\n",
       " u'casual',\n",
       " u'category',\n",
       " u'cauliflower',\n",
       " u'cause',\n",
       " u'causes',\n",
       " u'celebration',\n",
       " u'certainly',\n",
       " u'ceviche',\n",
       " u'chaat',\n",
       " u'chair',\n",
       " u'chairs',\n",
       " u'chana',\n",
       " u'chance',\n",
       " u'change',\n",
       " u'changing',\n",
       " u'char',\n",
       " u'character',\n",
       " u'charge',\n",
       " u'charm',\n",
       " u'charred',\n",
       " u'cheap',\n",
       " u'cheaper',\n",
       " u'check',\n",
       " u'checked',\n",
       " u'checks',\n",
       " u'cheese',\n",
       " u'cheesecake',\n",
       " u'cheesy',\n",
       " u'chef',\n",
       " u'chewy',\n",
       " u'chicago',\n",
       " u'chicken',\n",
       " u'children',\n",
       " u'chili',\n",
       " u'chilled',\n",
       " u'china',\n",
       " u'chinatown',\n",
       " u'chinatowns',\n",
       " u'chinese',\n",
       " u'choice',\n",
       " u'choices',\n",
       " u'choose',\n",
       " u'chopped',\n",
       " u'chops',\n",
       " u'chopsticks',\n",
       " u'chose',\n",
       " u'chow',\n",
       " u'choy',\n",
       " u'chunks',\n",
       " u'chutneys',\n",
       " u'cilantro',\n",
       " u'circular',\n",
       " u'circumstances',\n",
       " u'circuses',\n",
       " u'cities',\n",
       " u'city',\n",
       " u'classy',\n",
       " u'clean',\n",
       " u'cleaner',\n",
       " u'cleanest',\n",
       " u'cleanliness',\n",
       " u'close',\n",
       " u'closed',\n",
       " u'closest',\n",
       " u'closet',\n",
       " u'clouds',\n",
       " u'clumsily',\n",
       " u'co',\n",
       " u'coated',\n",
       " u'coating',\n",
       " u'coffee',\n",
       " u'cohorts',\n",
       " u'cold',\n",
       " u'coleslaw',\n",
       " u'colleague',\n",
       " u'collect',\n",
       " u'combination',\n",
       " u'combine',\n",
       " u'come',\n",
       " u'comes',\n",
       " u'comfortable',\n",
       " u'comforting',\n",
       " u'coming',\n",
       " u'communal',\n",
       " u'companion',\n",
       " u'companions',\n",
       " u'comparable',\n",
       " u'compare',\n",
       " u'compared',\n",
       " u'compares',\n",
       " u'comparing',\n",
       " u'complained',\n",
       " u'complains',\n",
       " u'complaints',\n",
       " u'complete',\n",
       " u'completely',\n",
       " u'complex',\n",
       " u'complimentary',\n",
       " u'con',\n",
       " u'concession',\n",
       " u'concluded',\n",
       " u'confetti',\n",
       " u'cons',\n",
       " u'considering',\n",
       " u'consistency',\n",
       " u'consistent',\n",
       " u'consists',\n",
       " u'constant',\n",
       " u'constantly',\n",
       " u'consume',\n",
       " u'consumption',\n",
       " u'contadino',\n",
       " u'contain',\n",
       " u'container',\n",
       " u'containers',\n",
       " u'contains',\n",
       " u'conversation',\n",
       " u'conversations',\n",
       " u'cooked',\n",
       " u'cool',\n",
       " u'corn',\n",
       " u'corner',\n",
       " u'corners',\n",
       " u'correctly',\n",
       " u'correlate',\n",
       " u'cost',\n",
       " u'costa',\n",
       " u'costco',\n",
       " u'could',\n",
       " u'couldn',\n",
       " u'couldnt',\n",
       " u'counter',\n",
       " u'couple',\n",
       " u'course',\n",
       " u'courses',\n",
       " u'coworkers',\n",
       " u'cozy',\n",
       " u'crab',\n",
       " u'craftsmanship',\n",
       " u'craving',\n",
       " u'crazy',\n",
       " u'creamy',\n",
       " u'creations',\n",
       " u'credit',\n",
       " u'creeping',\n",
       " u'cried',\n",
       " u'crispy',\n",
       " u'crowded',\n",
       " u'crowds',\n",
       " u'crunch',\n",
       " u'crust',\n",
       " u'crusty',\n",
       " u'ct',\n",
       " u'cubes',\n",
       " u'cucumber',\n",
       " u'cucumbers',\n",
       " u'cuisine',\n",
       " u'cumin',\n",
       " u'cup',\n",
       " u'curd',\n",
       " u'curly',\n",
       " u'currently',\n",
       " u'curries',\n",
       " u'curry',\n",
       " u'cury',\n",
       " u'cusine',\n",
       " u'customers',\n",
       " u'cut',\n",
       " u'cutting',\n",
       " u'cuz',\n",
       " u'd',\n",
       " u'daal',\n",
       " u'dad',\n",
       " u'dahl',\n",
       " u'dance',\n",
       " u'dangerous',\n",
       " u'dash',\n",
       " u'date',\n",
       " u'dated',\n",
       " u'day',\n",
       " u'deal',\n",
       " u'dealing',\n",
       " u'death',\n",
       " u'decent',\n",
       " u'decently',\n",
       " u'decide',\n",
       " u'decided',\n",
       " u'decision',\n",
       " u'decor',\n",
       " u'decorated',\n",
       " u'decoration',\n",
       " u'decorations',\n",
       " u'deeeelicious',\n",
       " u'deep',\n",
       " u'defiantly',\n",
       " u'definite',\n",
       " u'definitely',\n",
       " u'delectable',\n",
       " u'delicacies',\n",
       " u'delicious',\n",
       " u'deliciousness',\n",
       " u'delightful',\n",
       " u'delish',\n",
       " u'delivered',\n",
       " u'deliverer',\n",
       " u'delivery',\n",
       " u'denying',\n",
       " u'deserved',\n",
       " u'deserves',\n",
       " u'desks',\n",
       " u'despite',\n",
       " u'dessert',\n",
       " u'desserts',\n",
       " u'destitute',\n",
       " u'detail',\n",
       " u'determined',\n",
       " u'detract',\n",
       " u'devon',\n",
       " u'diarrhea',\n",
       " u'diced',\n",
       " u'did',\n",
       " u'didn',\n",
       " u'die',\n",
       " u'differ',\n",
       " u'difference',\n",
       " u'differences',\n",
       " u'different',\n",
       " u'difficult',\n",
       " u'digs',\n",
       " u'diluted',\n",
       " u'dim',\n",
       " u'dimension',\n",
       " u'din',\n",
       " u'dine',\n",
       " u'diners',\n",
       " u'ding',\n",
       " u'dining',\n",
       " u'dinner',\n",
       " u'disappointed',\n",
       " u'disappointingly',\n",
       " u'disappointment',\n",
       " u'discount',\n",
       " u'discover',\n",
       " u'disgusting',\n",
       " u'dish',\n",
       " u'dishes',\n",
       " u'disliked',\n",
       " u'disorganization',\n",
       " u'display',\n",
       " u'disposable',\n",
       " u'distinctive',\n",
       " u'distinguish',\n",
       " u'disturb',\n",
       " u'divorce',\n",
       " u'do',\n",
       " u'does',\n",
       " u'dollar',\n",
       " u'dollars',\n",
       " u'dominant',\n",
       " u'don',\n",
       " u'done',\n",
       " u'door',\n",
       " u'doordash',\n",
       " u'double',\n",
       " u'doufu',\n",
       " u'dough',\n",
       " u'doughnut',\n",
       " u'doughy',\n",
       " u'down',\n",
       " u'downstairs',\n",
       " u'downtown',\n",
       " u'dozen',\n",
       " u'drafty',\n",
       " u'dragons',\n",
       " u'drank',\n",
       " u'drawbacks',\n",
       " u'drenched',\n",
       " u'dried',\n",
       " u'drink',\n",
       " u'drinks',\n",
       " u'drip',\n",
       " u'drive',\n",
       " u'driver',\n",
       " u'drk',\n",
       " u'drop',\n",
       " u'drums',\n",
       " u'drunken',\n",
       " u'dry',\n",
       " u'dtf',\n",
       " u'duck',\n",
       " u'due',\n",
       " u'duffield',\n",
       " u'dumpling',\n",
       " u'dumplings',\n",
       " u'duration',\n",
       " u'during',\n",
       " u'e',\n",
       " u'each',\n",
       " u'eagerly',\n",
       " u'early',\n",
       " u'earned',\n",
       " u'earth',\n",
       " u'easily',\n",
       " u'east',\n",
       " u'eat',\n",
       " u'eaten',\n",
       " u'eating',\n",
       " u'edgar',\n",
       " u'edge',\n",
       " u'edgewater',\n",
       " u'edible',\n",
       " u'efficient',\n",
       " u'egg',\n",
       " u'eggs',\n",
       " u'ehh',\n",
       " u'eight',\n",
       " u'either',\n",
       " u'elaborate',\n",
       " u'else',\n",
       " u'elsewhere',\n",
       " u'employees',\n",
       " u'encounter',\n",
       " u'encouraged',\n",
       " u'end',\n",
       " u'ended',\n",
       " u'ends',\n",
       " u'england',\n",
       " u'english',\n",
       " u'enjoy',\n",
       " u'enjoyed',\n",
       " u'enjoying',\n",
       " u'enough',\n",
       " u'enter',\n",
       " u'entering',\n",
       " u'entire',\n",
       " u'entrance',\n",
       " u'entrees',\n",
       " u'environment',\n",
       " u'episode',\n",
       " u'equally',\n",
       " u'equate',\n",
       " u'especially',\n",
       " u'essentially',\n",
       " u'establish',\n",
       " u'establishments',\n",
       " u'etc',\n",
       " u'even',\n",
       " u'evening',\n",
       " u'ever',\n",
       " u'every',\n",
       " u'everyday',\n",
       " u'everyone',\n",
       " u'everything',\n",
       " u'exactly',\n",
       " u'example',\n",
       " u'excel',\n",
       " u'excellent',\n",
       " u'except',\n",
       " u'exception',\n",
       " u'exceptional',\n",
       " u'excited',\n",
       " u'executive',\n",
       " u'expect',\n",
       " u'expected',\n",
       " u'expectedly',\n",
       " u'expecting',\n",
       " u'expensive',\n",
       " u'experience',\n",
       " u'explaining',\n",
       " u'exploding',\n",
       " u'explore',\n",
       " u'extensive',\n",
       " u'extra',\n",
       " u'extraordinary',\n",
       " u'extremely',\n",
       " u'eyes',\n",
       " u'fabulous',\n",
       " u'face',\n",
       " u'fact',\n",
       " u'fail',\n",
       " u'fair',\n",
       " u'falls',\n",
       " u'familiar',\n",
       " u'family',\n",
       " u'famous',\n",
       " u'fan',\n",
       " u'fantastic',\n",
       " u'far',\n",
       " u'faraz',\n",
       " u'fare',\n",
       " u'fast',\n",
       " u'fat',\n",
       " u'fattier',\n",
       " u'fatty',\n",
       " u'favorite',\n",
       " u'favorites',\n",
       " u'feared',\n",
       " u'feast',\n",
       " u'fed',\n",
       " u'feel',\n",
       " u'feeling',\n",
       " u'fell',\n",
       " u'felt',\n",
       " u'female',\n",
       " u'fence',\n",
       " u'few',\n",
       " u'fianc',\n",
       " u'fidi',\n",
       " u'fiery',\n",
       " u'figure',\n",
       " u'filet',\n",
       " u'filets',\n",
       " u'fill',\n",
       " u'filled',\n",
       " u'fillet',\n",
       " u'filling',\n",
       " u'fills',\n",
       " u'finale',\n",
       " u'finally',\n",
       " u'find',\n",
       " u'fine',\n",
       " u'finest',\n",
       " u'finish',\n",
       " u'firmer',\n",
       " u'first',\n",
       " u'fish',\n",
       " u'fit',\n",
       " u'fits',\n",
       " u'five',\n",
       " u'fix',\n",
       " u'fixed',\n",
       " u'flaky',\n",
       " u'flat',\n",
       " u'flavor',\n",
       " u'flavorful',\n",
       " u'flavorless',\n",
       " u'flavors',\n",
       " u'flimsy',\n",
       " u'flop',\n",
       " u'flour',\n",
       " u'fluffy',\n",
       " u'flushing',\n",
       " u'focus',\n",
       " u'folks',\n",
       " u'food',\n",
       " u'foods',\n",
       " u'for',\n",
       " u'forced',\n",
       " u'forget',\n",
       " u'formal',\n",
       " u'forth',\n",
       " u'forward',\n",
       " u'found',\n",
       " u'four',\n",
       " u'foyer',\n",
       " u'fragrant',\n",
       " u'france',\n",
       " u'francisco',\n",
       " u'frankly',\n",
       " u'freak',\n",
       " u'free',\n",
       " u'fresh',\n",
       " u'freshly',\n",
       " u'friday',\n",
       " u'fried',\n",
       " u'friend',\n",
       " u'friendly',\n",
       " u'friends',\n",
       " u'fries',\n",
       " u'from',\n",
       " u'front',\n",
       " u'frontrunners',\n",
       " u'frozen',\n",
       " u'fry',\n",
       " u'fu',\n",
       " u'full',\n",
       " u'fun',\n",
       " u'fung',\n",
       " u'funky',\n",
       " u'funny',\n",
       " u'furnished',\n",
       " u'furthermore',\n",
       " u'fuse',\n",
       " u'fusion',\n",
       " u'future',\n",
       " u'fyi',\n",
       " u'ga',\n",
       " u'gagged',\n",
       " u'garaje',\n",
       " u'garlic',\n",
       " u'garlicky',\n",
       " u'garnished',\n",
       " u'gather',\n",
       " u'gatherings',\n",
       " u'gathers',\n",
       " u'gave',\n",
       " u'gelatin',\n",
       " u'gelatinous',\n",
       " u'gem',\n",
       " u'general',\n",
       " u'generally',\n",
       " u'generous',\n",
       " u'gentle',\n",
       " u'gently',\n",
       " u'gentrification',\n",
       " u'gentrifiers',\n",
       " u'get',\n",
       " u'gets',\n",
       " u'getting',\n",
       " u'giardiniera',\n",
       " u'ginger',\n",
       " u'girlfriend',\n",
       " u'give',\n",
       " u'given',\n",
       " u'gives',\n",
       " u'giving',\n",
       " u'glad',\n",
       " u'glistening',\n",
       " u'glory',\n",
       " u'gloves',\n",
       " u'glue',\n",
       " u'gluten',\n",
       " u'go',\n",
       " u'god',\n",
       " u'godsend',\n",
       " u'goes',\n",
       " u'going',\n",
       " u'gold',\n",
       " u'golden',\n",
       " u'gonna',\n",
       " u'good',\n",
       " u'goodness',\n",
       " u'gooey',\n",
       " u'gorai',\n",
       " u'got',\n",
       " u'gotta',\n",
       " u'gotten',\n",
       " u'grade',\n",
       " u'grass',\n",
       " u'gratuity',\n",
       " u'greasy',\n",
       " u'great',\n",
       " u'greatly',\n",
       " u'greek',\n",
       " u'green',\n",
       " u'grill',\n",
       " u'grilled',\n",
       " u'gross',\n",
       " u'grossed',\n",
       " u'ground',\n",
       " u'group',\n",
       " u'groupons',\n",
       " u'groups',\n",
       " u'grub',\n",
       " u'guess',\n",
       " u'gulab',\n",
       " u'gulped',\n",
       " u'guy',\n",
       " u'guys',\n",
       " u'had',\n",
       " u'hai',\n",
       " u'hair',\n",
       " u'half',\n",
       " u'halloween',\n",
       " u'hallway',\n",
       " u'hand',\n",
       " u'handful',\n",
       " u'handle',\n",
       " u'handling',\n",
       " u'hangry',\n",
       " u'happen',\n",
       " u'happened',\n",
       " u'happier',\n",
       " u'happy',\n",
       " u'hard',\n",
       " u'has',\n",
       " u'hasn',\n",
       " u'hatch',\n",
       " u'hate',\n",
       " u'have',\n",
       " u'haven',\n",
       " u'having',\n",
       " u'he',\n",
       " u'heading',\n",
       " u'health',\n",
       " u'hear',\n",
       " u'heard',\n",
       " u'hearty',\n",
       " u'heat',\n",
       " u'heaven',\n",
       " u'heavier',\n",
       " u'heavy',\n",
       " u'held',\n",
       " u'hella',\n",
       " u'helpful',\n",
       " u'her',\n",
       " u'here',\n",
       " u'hey',\n",
       " u'hi',\n",
       " u'high',\n",
       " u'higher',\n",
       " u'highlights',\n",
       " u'highly',\n",
       " u'hike',\n",
       " u'him',\n",
       " u'hipster',\n",
       " u'his',\n",
       " u'hit',\n",
       " u'hmmmm',\n",
       " u'hold',\n",
       " u'hole',\n",
       " u'holes',\n",
       " u'home',\n",
       " u'homemade',\n",
       " u'hometown',\n",
       " u'honest',\n",
       " u'honestly',\n",
       " u'hong',\n",
       " u'hood',\n",
       " u'hooray',\n",
       " u'hope',\n",
       " u'hoping',\n",
       " u'horrible',\n",
       " u'host',\n",
       " u'hostess',\n",
       " u'hot',\n",
       " u'hour',\n",
       " u'hours',\n",
       " u'house',\n",
       " u'how',\n",
       " u'however',\n",
       " u'hub',\n",
       " u'huge',\n",
       " u'hugging',\n",
       " u'humidity',\n",
       " u'hunger',\n",
       " u'hungry',\n",
       " u'husband',\n",
       " u'hustles',\n",
       " u'hype',\n",
       " u'i',\n",
       " u'ice',\n",
       " u'if',\n",
       " u'ignore',\n",
       " u'ignoring',\n",
       " u'im',\n",
       " u'imagined',\n",
       " u'immediate',\n",
       " u'immediately',\n",
       " u'implying',\n",
       " u'impolite',\n",
       " u'importantly',\n",
       " u'impossible',\n",
       " u'impressed',\n",
       " u'impressive',\n",
       " u'improved',\n",
       " u'in',\n",
       " u'included',\n",
       " u'includes',\n",
       " u'including',\n",
       " u'inclusion',\n",
       " u'incompetent',\n",
       " u'incredible',\n",
       " u'independently',\n",
       " u'india',\n",
       " u'indian',\n",
       " u'inedible',\n",
       " u'inefficient',\n",
       " u'ingredients',\n",
       " u'inside',\n",
       " u'insides',\n",
       " u'instagramming',\n",
       " u'instead',\n",
       " u'instructions',\n",
       " u'intact',\n",
       " u'interesting',\n",
       " u'interior',\n",
       " u'interrupted',\n",
       " u'intestines',\n",
       " u'intimate',\n",
       " u'into',\n",
       " ...]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first shuffled document manual_labels/train/pos/62.txt has label 1 and terms: [1, 4, 8, 10, 11, 16, 17, 20, 23, 27, 29, 35, 39, 40, 42, 46, 49, 54, 56, 63, 76, 87, 88, 98, 102, 106, 107, 110, 111, 115, 127, 129, 133, 135, 137, 139, 143, 145, 147, 151, 155, 168, 169, 180, 182, 189, 190, 193, 194, 198, 203, 213, 215, 219, 221, 222, 229, 233, 239, 252, 290, 292, 294, 297, 303, 304, 311, 312, 316, 323, 325, 336, 355, 356, 357, 363, 370, 373, 378, 391, 397, 398, 402, 438, 456, 473, 489, 497, 505, 515, 518, 525, 540, 545, 547, 555, 564, 578, 584, 587, 593, 597, 608, 611, 612, 614, 616, 621, 622, 624, 634, 642, 643, 649, 654, 655, 668, 683, 686, 703, 712, 724, 743, 744, 753, 760, 762, 763, 765, 772, 780, 782, 788, 815, 818, 821, 829, 835, 848, 849, 853, 860, 878, 885, 887, 889, 890, 894, 898, 907, 911, 915, 918, 942, 944, 953, 957, 959, 961, 967, 975, 978, 988, 989, 999, 1004, 1007, 1011, 1012, 1020, 1024, 1036, 1057, 1068, 1083, 1095, 1100, 1102, 1114, 1116, 1130, 1131, 1135, 1139, 1143, 1153, 1156, 1163, 1166, 1171, 1194, 1196, 1202, 1205, 1211, 1212, 1219, 1232, 1249, 1251, 1258, 1266, 1276, 1280, 1283, 1290, 1291, 1294, 1301, 1306, 1307, 1308, 1311, 1332, 1337, 1349, 1351, 1353, 1356, 1364, 1369, 1371, 1372, 1376, 1378, 1379, 1381, 1384, 1386, 1388, 1401, 1407, 1416, 1423, 1427, 1447, 1473, 1475, 1481, 1495, 1502, 1507, 1510, 1525, 1535, 1549, 1564, 1567, 1587, 1607, 1637, 1639, 1640, 1648, 1650, 1654, 1678, 1687, 1696, 1702, 1707, 1708, 1714, 1715, 1719, 1720, 1736, 1737, 1741, 1746, 1754, 1755, 1764, 1773, 1775, 1790, 1794, 1800, 1805, 1806, 1811, 1816, 1819, 1820, 1834, 1835, 1836, 1839, 1857, 1858, 1863, 1876, 1890, 1891, 1897, 1903, 1906, 1923, 1931, 1948, 1956, 1957, 1958, 1965, 1978, 1987, 1989, 2002, 2004, 2006, 2007, 2008, 2010, 2011, 2012, 2013, 2014, 2019, 2021, 2022, 2026, 2028, 2031, 2038, 2042, 2046, 2049, 2052, 2056, 2058, 2060, 2061, 2062, 2068, 2089, 2094, 2097, 2099, 2100, 2105, 2110, 2118, 2124, 2133, 2142, 2150, 2155, 2158, 2161, 2170, 2172, 2178, 2179, 2180, 2184, 2185, 2186, 2197, 2199, 2204, 2209, 2210, 2217, 2234, 2247, 2249, 2255, 2259, 2263, 2275, 2284, 2285, 2287, 2291]\n"
     ]
    }
   ],
   "source": [
    "def repeatable_random(seed):\n",
    "    hash = str(seed)\n",
    "    while True:\n",
    "        hash = hashlib.md5(hash).digest()\n",
    "        for c in hash:\n",
    "            yield ord(c)\n",
    "\n",
    "def repeatable_shuffle(X, y, filenames):\n",
    "    r = repeatable_random(42) \n",
    "    indices = sorted(range(X.shape[0]), key=lambda x: next(r))\n",
    "    return X[indices], y[indices], np.array(filenames)[indices]\n",
    "\n",
    "X, y, filenames = repeatable_shuffle(matrix, labels, all_train_files)\n",
    "\n",
    "print('first shuffled document %s has label %d and terms: %s' % \n",
    "      (filenames[0], y[0], sorted(X[0].nonzero()[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_clf():\n",
    "    return LogisticRegression(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-25-79e9801650a8>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-25-79e9801650a8>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    print \"fold\", c ,\"accuracy=\",score\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def do_cross_validation(X, y, n_folds=5, verbose=False):\n",
    "    kf = KFold(X.shape[0],n_folds,shuffle=False,random_state=None)\n",
    "    c = tot =0\n",
    "    clf = get_clf()\n",
    "    for k, (train, test) in enumerate(kf):\n",
    "        clf.fit(X[train], y[train])\n",
    "        score = accuracy_score(y[test], clf.predict(X[test]))\n",
    "        if (verbose):\n",
    "            print \"fold\", c ,\"accuracy=\",score\n",
    "            c+=1\n",
    "        tot += score\n",
    "    return (1.* tot)/ n_folds\n",
    "    \n",
    "print('average cross validation accuracy=%.4f' %\n",
    "      do_cross_validation(X, y, verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_expt(filenames, y, tokenizer_fn=tokenize,\n",
    "            min_df=1, max_df=1., binary=True,\n",
    "            ngram_range=(1,1), n_folds=5):\n",
    "    return do_cross_validation(do_vectorize(filenames.tolist(),binary=binary,tokenizer_fn=tokenizer_fn,min_df=min_df,max_df=max_df)[0], y,n_folds=n_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'do_cross_validation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-99ba853bc25b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'accuracy using default settings: %.4g'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdo_expt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-26-83eec2f97fc4>\u001b[0m in \u001b[0;36mdo_expt\u001b[0;34m(filenames, y, tokenizer_fn, min_df, max_df, binary, ngram_range, n_folds)\u001b[0m\n\u001b[1;32m      2\u001b[0m             \u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m             ngram_range=(1,1), n_folds=5):\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdo_cross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo_vectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_folds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_folds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: global name 'do_cross_validation' is not defined"
     ]
    }
   ],
   "source": [
    "print('accuracy using default settings: %.4g' % do_expt(filenames, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'do_cross_validation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-9b25ed096429>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mcompare_n_folds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# considering 50 folds for later experiments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-9b25ed096429>\u001b[0m in \u001b[0;36mcompare_n_folds\u001b[0;34m(filenames, y)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompare_n_folds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m70\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdo_expt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_folds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'n_folds'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-83eec2f97fc4>\u001b[0m in \u001b[0;36mdo_expt\u001b[0;34m(filenames, y, tokenizer_fn, min_df, max_df, binary, ngram_range, n_folds)\u001b[0m\n\u001b[1;32m      2\u001b[0m             \u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m             ngram_range=(1,1), n_folds=5):\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdo_cross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo_vectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_folds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_folds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: global name 'do_cross_validation' is not defined"
     ]
    }
   ],
   "source": [
    "def compare_n_folds(filenames, y):\n",
    "    arr = [2,5,10,20,30,40,50,60,70]\n",
    "    val = [do_expt(filenames, y,n_folds=i) for i in arr]\n",
    "    plt.figure()\n",
    "    plt.xlabel('n_folds')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.plot(arr,val,'bo-')\n",
    "    return val\n",
    "    \n",
    "compare_n_folds(filenames, y)\n",
    "\n",
    "# considering 50 folds for later experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'do_cross_validation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-8c2a32929670>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdo_expt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdo_expt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcompare_binary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-8c2a32929670>\u001b[0m in \u001b[0;36mcompare_binary\u001b[0;34m(filenames, y)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompare_binary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdo_expt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdo_expt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcompare_binary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-83eec2f97fc4>\u001b[0m in \u001b[0;36mdo_expt\u001b[0;34m(filenames, y, tokenizer_fn, min_df, max_df, binary, ngram_range, n_folds)\u001b[0m\n\u001b[1;32m      2\u001b[0m             \u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m             ngram_range=(1,1), n_folds=5):\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdo_cross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo_vectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_folds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_folds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: global name 'do_cross_validation' is not defined"
     ]
    }
   ],
   "source": [
    "def compare_binary(filenames, y):\n",
    "    return [do_expt(filenames, y,binary=True),do_expt(filenames, y,binary=False)]\n",
    "          \n",
    "compare_binary(filenames, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'do_cross_validation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-f8c0d7a37fe0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdo_expt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_train_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdo_expt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_train_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenize_with_punct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdo_expt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_train_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenize_with_not\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtokenizer_expt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-30-f8c0d7a37fe0>\u001b[0m in \u001b[0;36mtokenizer_expt\u001b[0;34m(all_train_files, y)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtokenizer_expt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_train_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdo_expt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_train_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdo_expt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_train_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenize_with_punct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdo_expt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_train_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenize_with_not\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtokenizer_expt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-83eec2f97fc4>\u001b[0m in \u001b[0;36mdo_expt\u001b[0;34m(filenames, y, tokenizer_fn, min_df, max_df, binary, ngram_range, n_folds)\u001b[0m\n\u001b[1;32m      2\u001b[0m             \u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m             ngram_range=(1,1), n_folds=5):\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdo_cross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo_vectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_folds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_folds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: global name 'do_cross_validation' is not defined"
     ]
    }
   ],
   "source": [
    "def tokenizer_expt(all_train_files, y):\n",
    "    return [do_expt(all_train_files, y,tokenizer_fn=tokenize),do_expt(all_train_files, y,tokenizer_fn=tokenize_with_punct),do_expt(all_train_files, y,tokenizer_fn=tokenize_with_not)]\n",
    "\n",
    "tokenizer_expt(filenames, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'do_cross_validation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-80d6fc35eece>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmin_df_expt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-31-80d6fc35eece>\u001b[0m in \u001b[0;36mmin_df_expt\u001b[0;34m(filenames, y)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmin_df_expt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmindf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdo_expt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmindf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'min_df'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-83eec2f97fc4>\u001b[0m in \u001b[0;36mdo_expt\u001b[0;34m(filenames, y, tokenizer_fn, min_df, max_df, binary, ngram_range, n_folds)\u001b[0m\n\u001b[1;32m      2\u001b[0m             \u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m             ngram_range=(1,1), n_folds=5):\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdo_cross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo_vectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_folds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_folds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: global name 'do_cross_validation' is not defined"
     ]
    }
   ],
   "source": [
    "def min_df_expt(filenames, y):\n",
    "    mindf = range(1,11)\n",
    "    val = [do_expt(filenames, y,min_df=i,tokenizer_fn=tokenize) for i in mindf]\n",
    "    plt.figure()\n",
    "    plt.xlabel('min_df')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.plot(mindf,val,'bo-')\n",
    "    return val\n",
    "\n",
    "min_df_expt(filenames, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-32-bf08a50ef9de>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-32-bf08a50ef9de>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    print maxdf\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def max_df_expt(filenames, y):\n",
    "    maxdf = np.linspace(0.1,1,10)\n",
    "    print maxdf\n",
    "    val = [do_expt(filenames, y,min_df=6,tokenizer_fn=tokenize,max_df=i) for i in maxdf]\n",
    "    plt.figure()\n",
    "    plt.xlabel('max_df')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.plot(maxdf,val,'bo-')\n",
    "    return val\n",
    "    \n",
    "max_df_expt(filenames, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
       "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"So, based on the above experiments, we set:\n",
    "binary=True\n",
    "tokenizer=tokenize\n",
    "min_df=6\n",
    "max_df=1.\"\"\"\n",
    "\n",
    "X, vec = do_vectorize(filenames, tokenizer_fn=tokenize,\n",
    "                      binary=True, min_df=6, max_df=1.)\n",
    "clf = get_clf()\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-34-27b338df0748>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-34-27b338df0748>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    print type(clf.coef_)\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "print type(clf.coef_)\n",
    "print clf.coef_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.38535502 -0.16709321  0.06644954  0.04920645 -0.02750608 -0.06655038\n",
      "  0.20261799 -0.06001036  0.10430076  0.09163123]\n",
      "[u'10', u'2', u'3', u'4', u'5', u'50', u'6', u'8', u'a', u'about']\n"
     ]
    }
   ],
   "source": [
    "print(clf.coef_[0][:10])\n",
    "print(vec.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top positive coefs: [(u'definitely', 0.840714144602624), (u'great', 0.8309602535321319), (u'ever', 0.7905831652749882), (u'the', 0.6043520265517477), (u'very', 0.5551453772535825)]\n",
      "top negative coefs: [(u'not', -0.8964606207435467), (u'on', -0.6371309514160892), (u'nothing', -0.5890143378280941), (u'taste', -0.5074490348623851), (u'bit', -0.48402064120734045)]\n"
     ]
    }
   ],
   "source": [
    "def get_top_coefficients(clf, vec, n=10):\n",
    "    sorted_tups = sorted(zip(vec.get_feature_names(),clf.coef_[0].tolist()), key=lambda tup: tup[1],reverse=True)\n",
    "    return sorted_tups[:5],sorted_tups[:-6:-1]\n",
    "\n",
    "pos_coef, neg_coef = get_top_coefficients(clf, vec, n=5)\n",
    "print('top positive coefs: %s' % str(pos_coef))\n",
    "print('top negative coefs: %s' % str(neg_coef))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test represents 33 documents with 325 features\n",
      "y_test has 26 positive and 7 negative labels\n",
      "first testing file is manual_labels/test/pos/100.txt\n",
      "last testing file is manual_labels/test/neg/97.txt\n",
      "testing accuracy=0.7878787879\n"
     ]
    }
   ],
   "source": [
    "X_test = vec.transform(all_test_files)\n",
    "y_test = np.array([1] * len(pos_test_files) + [0] * len(neg_test_files))\n",
    "print('X_test represents %d documents with %d features' % (X_test.shape[0], X_test.shape[1]))\n",
    "print('y_test has %d positive and %d negative labels' % (len(np.where(y_test==1)[0]),\n",
    "                                                          len(np.where(y_test==0)[0])))\n",
    "print('first testing file is %s' % all_test_files[0])\n",
    "print('last testing file is %s' % all_test_files[-1])\n",
    "print('testing accuracy=%.10g' % accuracy_score(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "272"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def index_of_term(vec, term):\n",
    "    return vec.get_feature_names().index(term)\n",
    "\n",
    "index_of_term(vec, 'the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing accuracy=0.7878787879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/knarisetti/anaconda/lib/python2.7/site-packages/scipy/sparse/compressed.py:730: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  SparseEfficiencyWarning)\n"
     ]
    }
   ],
   "source": [
    "def train_after_removing_features(X, y, vec, features_to_remove):\n",
    "    listofcols = [index_of_term(vec, f) for f in features_to_remove]\n",
    "    for i in listofcols:\n",
    "        X[:,i] = 0\n",
    "    clf = get_clf()\n",
    "    clf.fit(X,y)\n",
    "    return clf\n",
    "    \n",
    "clf = train_after_removing_features(X.copy(), y, vec, ['the'])\n",
    "print('testing accuracy=%.10g' % accuracy_score(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'filename': 'manual_labels/test/neg/105.txt',\n",
       "  'index': 26,\n",
       "  'predicted': 1,\n",
       "  'probas': array([ 0.4040528,  0.5959472]),\n",
       "  'truth': 0},\n",
       " {'filename': 'manual_labels/test/neg/122.txt',\n",
       "  'index': 27,\n",
       "  'predicted': 1,\n",
       "  'probas': array([ 0.4040528,  0.5959472]),\n",
       "  'truth': 0},\n",
       " {'filename': 'manual_labels/test/neg/148.txt',\n",
       "  'index': 28,\n",
       "  'predicted': 1,\n",
       "  'probas': array([ 0.4040528,  0.5959472]),\n",
       "  'truth': 0},\n",
       " {'filename': 'manual_labels/test/neg/40.txt',\n",
       "  'index': 29,\n",
       "  'predicted': 1,\n",
       "  'probas': array([ 0.4040528,  0.5959472]),\n",
       "  'truth': 0},\n",
       " {'filename': 'manual_labels/test/neg/84.txt',\n",
       "  'index': 30,\n",
       "  'predicted': 1,\n",
       "  'probas': array([ 0.4040528,  0.5959472]),\n",
       "  'truth': 0},\n",
       " {'filename': 'manual_labels/test/neg/94.txt',\n",
       "  'index': 31,\n",
       "  'predicted': 1,\n",
       "  'probas': array([ 0.4040528,  0.5959472]),\n",
       "  'truth': 0},\n",
       " {'filename': 'manual_labels/test/neg/97.txt',\n",
       "  'index': 32,\n",
       "  'predicted': 1,\n",
       "  'probas': array([ 0.4040528,  0.5959472]),\n",
       "  'truth': 0}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_top_errors(X_test, y_test, filenames, clf, n=10):\n",
    "    truth_array = clf.predict(X_test)\n",
    "    prob_list = clf.predict_proba(X_test)\n",
    "    mis_classified = sorted([{'filename':filenames[i],'index':i,'predicted':truth_array[i],'probas':prob_list[i][truth_array[i]],'truth':y_test[i]} for i,t in enumerate(truth_array) if t != y_test[i]],key=lambda d: d['probas'],reverse=True)[:10]\n",
    "    for m in mis_classified:\n",
    "        m['probas'] = prob_list[m['index']]\n",
    "    return mis_classified\n",
    "    \n",
    "errors = get_top_errors(X_test, y_test, all_test_files, clf)\n",
    "errors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
